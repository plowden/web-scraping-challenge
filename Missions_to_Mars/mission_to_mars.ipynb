{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a browser object for use by all the scrapes.\n",
    "def init_browser():\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape https://mars.nasa.gov/news/ and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_mars_news():\n",
    "    browser = init_browser()\n",
    "\n",
    "    # Visit https://mars.nasa.gov/news/\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Give time for dynamic content to load\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    #DEBUG    print(soup.prettify())\n",
    "    #DEBUG    list(soup.children)\n",
    "\n",
    "    # Get the title\n",
    "    scrape_title = soup.find(class_='content_title')\n",
    "    title = scrape_title.find_all('a')[0].text\n",
    "\n",
    "    # Get the teaser\n",
    "    scrape_teaser = soup.find('div', class_='article_teaser_body')\n",
    "    teaser = scrape_teaser.get_text()\n",
    "\n",
    "    # Store data in a dictionary\n",
    "    mars_news = {\n",
    "        \"title\": title,\n",
    "        \"teaser\": teaser\n",
    "    }\n",
    "\n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "\n",
    "    # Return results\n",
    "    return mars_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Year of Surprising Science From NASA's InSight Mars Mission\n",
      "A batch of new papers summarizes the lander's findings above and below the surface of the Red Planet.\n"
     ]
    }
   ],
   "source": [
    "mars_news = scrape_mars_news()\n",
    "print(f'Title: {mars_news[\"title\"]}')\n",
    "print(f'Teaser: {mars_news[\"teaser\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visit the url for JPL Featured Space Image at https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars. Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`. Make sure to find the image url to the full size `.jpg` image. Make sure to save a complete url string for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_featured_image():\n",
    "    browser = init_browser()\n",
    "\n",
    "    # Format: https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA00063_hires.jpg\n",
    "    # Visit https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\n",
    "    url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    base_url = \"https://www.jpl.nasa.gov\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Give time for dynamic content to load\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Get the title\n",
    "    scraped_url = soup.find('a', id='full_image')\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for tmp in soup.find_all('a', {'class':'button fancybox'}):\n",
    "        results.append(tmp['data-fancybox-href'])\n",
    "        #DEBUG  print (results)\n",
    "\n",
    "    featured_image_url = base_url + results[0] \n",
    "    \n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "\n",
    "    # Return results\n",
    "    return featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/spaceimages/images/mediumsize/PIA17932_ip.jpg']\n",
      "Featured Image URL: https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA17932_ip.jpg\n"
     ]
    }
   ],
   "source": [
    "featured_image_url = scrape_featured_image()\n",
    "print(f'Featured Image URL: {featured_image_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visit the Mars Weather twitter account at https://twitter.com/marswxreport?lang=en and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called `mars_weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweet():\n",
    "    browser = init_browser()\n",
    "\n",
    "    #span.css-901oao.css-16my406.css-bfa6kz.r-1qd0xha.r-ad9z0x.r-bcqeeo.r-qvutc0\n",
    "    \n",
    "    #<span class=\"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\">InSight</span>\n",
    "\n",
    "    # Visit https://twitter.com/marswxreport?lang=en\n",
    "    url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Give time for dynamic content to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Get the title\n",
    "    #scraped_tweet = soup.find('span', class_='css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0')\n",
    "    #print(scraped_tweet)\n",
    "    #results = []\n",
    "    scraped_tweet = soup.find_all(text=True)\n",
    "    #print(scraped_tweet)\n",
    "\n",
    "    #for tmp in soup.find_all('a', {'class':'button fancybox'}):\n",
    "    #    results.append(tmp['data-link'])\n",
    "        #DEBUG print (results)\n",
    "    \n",
    "    tweet = ''\n",
    "    for text in scraped_tweet:\n",
    "        #print(f'text: {text}')\n",
    "        if re.search(r'^InSight', text):\n",
    "            #print(f'tweet: {text}')\n",
    "            tweet = text\n",
    "            break\n",
    "        \n",
    "    #tweet = scraped_tweet\n",
    "        \n",
    "    #x = scraped_tweet['class']\n",
    "    #print(x)\n",
    "    #txt = '.'.join(x)\n",
    "    #txt = 'span' + '.' + txt\n",
    "    #txt = x[0]\n",
    "    #print(txt)\n",
    "\n",
    "    #DEBUG        print(soup.prettify())\n",
    "    #DEBUG    list(soup.children)\n",
    "    \n",
    "        # Visit https://twitter.com/marswxreport?lang=en\n",
    "    #url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    #browser.visit(url)\n",
    "\n",
    "    # Give time for dynamic content to load\n",
    "    #time.sleep(5)\n",
    "\n",
    "    # Scrape page into Soup\n",
    "    #html = browser.html\n",
    "    #soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    #new_scraped_tweet = soup.select_one(txt)\n",
    "\n",
    "\n",
    "    #tweet = scraped_tweet.next_sibling\n",
    "    #tweet = new_scraped_tweet.text\n",
    "    #print(tweet)\n",
    "    \n",
    "    # Close the browser after scraping\n",
    "    browser.quit()\n",
    "\n",
    "    # Return results\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: InSight sol 443 (2020-02-24) low -94.8ºC (-138.6ºF) high -12.3ºC (9.8ºF)\n",
      "winds from the SSE at 7.0 m/s (15.6 mph) gusting to 22.2 m/s (49.6 mph)\n",
      "pressure at 6.30 hPa\n"
     ]
    }
   ],
   "source": [
    "mars_weather = scrape_tweet()\n",
    "print(f'Tweet: {mars_weather}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visit the Mars Facts webpage at https://space-facts.com/mars/ and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc. Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit https://space-facts.com/mars/ \n",
    "url = \"https://space-facts.com/mars/\"\n",
    "\n",
    "# Read in tables.\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Convert first table to dataframe.\n",
    "df = tables[0]\n",
    "\n",
    "# Store dataframe as an HTML table string.\n",
    "html_table = df.to_html()\n",
    "\n",
    "# Strip newline characters.\n",
    "html_table = html_table.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>0</th>      <th>1</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Equatorial Diameter:</td>      <td>6,792 km</td>    </tr>    <tr>      <th>1</th>      <td>Polar Diameter:</td>      <td>6,752 km</td>    </tr>    <tr>      <th>2</th>      <td>Mass:</td>      <td>6.39 × 10^23 kg (0.11 Earths)</td>    </tr>    <tr>      <th>3</th>      <td>Moons:</td>      <td>2 (Phobos &amp; Deimos)</td>    </tr>    <tr>      <th>4</th>      <td>Orbit Distance:</td>      <td>227,943,824 km (1.38 AU)</td>    </tr>    <tr>      <th>5</th>      <td>Orbit Period:</td>      <td>687 days (1.9 years)</td>    </tr>    <tr>      <th>6</th>      <td>Surface Temperature:</td>      <td>-87 to -5 °C</td>    </tr>    <tr>      <th>7</th>      <td>First Record:</td>      <td>2nd millennium BC</td>    </tr>    <tr>      <th>8</th>      <td>Recorded By:</td>      <td>Egyptian astronomers</td>    </tr>  </tbody></table>'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visit the USGS Astrogeology site at https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres. You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image. Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`. Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = init_browser()\n",
    "\n",
    "# Visit https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars \n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)\n",
    "\n",
    "# Give time for dynamic content to load\n",
    "time.sleep(5)\n",
    "    \n",
    "# HTML object\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')\n",
    "# Retrieve all elements that contain book information\n",
    "hemispheres = soup.find_all('div', class_='description')\n",
    "#print(hemispheres)\n",
    "\n",
    "hemisphere_list = []\n",
    "hemisphere_dict = {}\n",
    "    \n",
    "# Iterate through each hemisphere\n",
    "for hemisphere in hemispheres:\n",
    "    # Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "    #print(hemisphere)\n",
    "    link = hemisphere.find('a')\n",
    "    #print(f'LINK: {link}')\n",
    "    title = link.find('h3').text.strip()\n",
    "    #href = re.search(r'href=\"([^\"]+)\".*', link)\n",
    "    href = re.search('\\/(.*)\\\"', str(link)).group(1)\n",
    "    link = 'https://astrogeology.usgs.gov/' + href\n",
    "    #print(f'HREF: {href}')\n",
    "    #print('-----------')\n",
    "    #print(title)\n",
    "    #http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg\n",
    "    #print(link)\n",
    "    \n",
    "    browser.visit(link)\n",
    "\n",
    "    # Give time for dynamic content to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # HTML object\n",
    "    html = browser.html\n",
    "\n",
    "    # Parse HTML with Beautiful Soup\n",
    "    soup = bs(html, 'html.parser')\n",
    "    div = soup.find('div', class_='downloads')\n",
    "    a = div.find('a')\n",
    "    img_url = a['href']\n",
    "\n",
    "    hemisphere_dict = {'title': title, 'img_url': img_url}\n",
    "    hemisphere_list.append(hemisphere_dict)\n",
    "        \n",
    "    #print(f'IMAGE_LINK: {href}')\n",
    "    \n",
    "    # Click the 'Next' button on each page\n",
    "    #try:\n",
    "    #    browser.click_link_by_partial_text('next')\n",
    "          \n",
    "    #except:\n",
    "    #    print(\"Scraping Complete\")\n",
    "\n",
    "# Close the browser after scraping\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hemisphere_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: MongoDB and Flask Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above. Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, create a route called `/scrape` that will import your `scrape_mars.py` script and call your `scrape` function. Store the return value in Mongo as a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a root route `/` that will query your Mongo database and pass the mars data into an HTML template to display the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a template HTML file called `index.html` that will take the mars data dictionary and display all of the data in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook(s), screenshots of your final application, link to your new repository to BootCampSpot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
